\chapter{Thesis Contribution}\label{ch:ThesisCont}
      
\section{Topic Modelling}\label{sec:TopicModelling}

In order to evaluate the relative importance of policy and party leaders in
driving political engagement on Twitter, all the tweets collected must first be
organized by topic. In order to do so, a latent Dirichlet allocation (LDA) was
trained on the English tweets of Canada's five major, english speaking party
leaders: Andrew Scheer, Elizabeth May, Jagmeet Singh, Justin Trudeau, and Maxime
Bernier. The timeframe of collection ranges from October 21, 2018 to October 21,
2019 - the eve of Canada's federal election. While the tweets from each Federal
party's official accounts were also collected, they predominantly acted as
logistical tools -- informing party affiliates of events and rallies. The
personal accounts for party leaders were generally more pertinent to their
beliefs, platforms and style of rhetoric, and thus are better suited in this
context. In this spirit, only tweets of the party leader were collected,
excluding retweets. Figure \ref{fig:tweets_over_time} visualizes the daily and
cumulative number of tweets over time, in aggregate and by party leader,
resulting in 7978 total tweets.

\begin{singlespacing}
    \begin{figure}[H]
    \centering
    \includegraphics[scale=0.40]{Figures/tweets_over_time}
    \caption[Daily and Cumulative Tweets over Time]{Daily and Cumulative Tweets over Time}
    \label{fig:tweets_over_time}
    \end{figure}
\end{singlespacing}

\subsubsection{Text Cleaning}

Given the inherent noise and extraneous info in text data, it is standard and
necessary to preprocess text before modelling \cite{sapul2017trending}. The text
cleaning pipeline removes punctuation marks, stop words, words with fewer than
three characters, and URLS, as well as common twitter symbols like ``RT:'',
``@'' and ``\#''. Emojis were converted to text using the python package
\texttt{emoji}. After this process, all text was converted to lower-case and
lemmatized to get rid of common suffixes. Therefore the tweet in figure
\ref{fig:tweet_ex} after preprocessing reads: \emph{wherever maple leaf fly
represents rich history bright future value hold dear happy flag day canada}.

\begin{singlespacing}
    \begin{figure}[H]
    \centering
    \includegraphics[scale=0.55]{Figures/tweet_ex}
    \caption[Example Tweet]{Example Tweet}
    \label{fig:tweet_ex}
    \end{figure}
\end{singlespacing}

\subsection{Hyper-Parameter Tuning}\label{sec:TopicModellingHP}

As discussed in section \ref{ch:TopicModelling}, the LDA takes in three
parameters: $\alpha$ - which acts as a concentration parameter for how documents
are modelled as topics; $\beta$ - which acts as a concentration parameter for
how topics are modelled as words; and $k$ which is the number of topics to be
modelled. By performing a parameter sweep, where $\alpha$ and $\beta$ lie on the
interval $\left[0,1\right]$ with increments of 0.05, and $k$ ranges between 4
and 7, the LDA was exposed to the entire corpus and then evaluated using c\_v
coherence. Figure \ref{fig:lda_param_sweep} shows, for each $k$ value, the c\_v
coherence as a function of different combinations of $\alpha$ and $\beta$.

\begin{singlespacing}
\begin{figure}
    \centering
    \begin{tabular}{cc}
      \includegraphics[width=65mm]{Figures/Coherence_Surface_k=4} &
      \includegraphics[width=65mm]{Figures/Coherence_Surface_k=5} \\
    (a) $k=4$ & (b) $k=5$ \\[6pt]
     \includegraphics[width=65mm]{Figures/Coherence_Surface_k=6} &
     \includegraphics[width=65mm]{Figures/Coherence_Surface_k=7} \\
    (c) $k=6$ & (d) $k=7$ \\[6pt]
    \end{tabular}
    \caption[LDA Parameter Sweep Results]{LDA Parameter Sweep Results}
    \label{fig:lda_param_sweep}
\end{figure}
\end{singlespacing}


\subsection{Results}\label{sec:TopicModellingRes}

After performing the parameter sweep described in section
\ref{sec:TopicModellingHP}, the most performant model had a $k$ value of 7,
$\alpha$ of 0.31 and $\beta$ of 0.81 and a c\_v coherence score of 0.48. By
labelling each tweet as the maximum probability value in its topic mixture, each
tweet was assigned a single topic. The word clouds for each topic are described
in figure \ref{fig:topic_word_clouds}. 

\begin{singlespacing}
    \begin{figure}
        \centering
        \begin{tabular}{ccc}
        \includegraphics[width=45mm]{Figures/topic_1_wordcloud} &
        \includegraphics[width=45mm]{Figures/topic_2_wordcloud} &
        \includegraphics[width=45mm]{Figures/topic_3_wordcloud} \\
        (a) Topic 1 & (b) Topic 2 & (c) Topic 3  \\[6pt]
        \includegraphics[width=45mm]{Figures/topic_4_wordcloud} &
        \includegraphics[width=45mm]{Figures/topic_5_wordcloud} &
        \includegraphics[width=45mm]{Figures/topic_6_wordcloud} \\
        (d) Topic 4 & (e) Topic 5 & (f) Topic 6  \\[6pt]
        \multicolumn{3}{c}{\includegraphics[width=45mm]{Figures/topic_7_wordcloud}
        }\\
        \multicolumn{3}{c}{(g) Topic 7}
        \end{tabular}
        \caption[LDA Topic Word Clouds]{LDA Topic Word Clouds}
        \label{fig:topic_word_clouds}
    \end{figure}
\end{singlespacing}

Topic 1 pertained to campaign messages, rallies and logistics -- and makes up
8.2\% of all tweets. Topic 2 contains tweets regarding a carbon tax, pipelines
and the economy -- and makes up 16.3\% of all tweets. Topic 3 contains tweets
about the SNC Lavalin affair, a scandal that plagued Justin Trudeau, and tweets
about corruption -- making up 18\% of all tweets. Topic 4 is predominantly
tweets appealing to the middle-class and economy -- and is 29.7\% of all tweets;
topic 5 contains celebratory messages about the campaign, as well as tweets
regarding national holidays and days of remembrance -- and make up 15\% of all
tweets. Topic 6 is made up of tweest about immigration, diversity and free
speech -- and makes up 11.5\% of all tweets. Finally, topic 7 contains
tweets regarding healthcare, abortion and pharmacare -- and makes up 1\% of all
tweets. The magnitude of how many tweets were assigned to each topic is shown in
figure \ref{fig:topic_distribution}. The vertices representing tweets of different
topics in figure \ref{fig:og_graph} are assigned different colours. 

\begin{singlespacing}
    \begin{figure}[H]
    \centering
    \includegraphics[scale=0.2]{Figures/topic_distribution}
    \caption[LDA Topic Distribution]{LDA Topic Distribution}
    \label{fig:topic_distribution}
    \end{figure}
\end{singlespacing}

\input{TopicCentrality}

\input{StochasticBlockModels}

